# bigdataprinciples1_2

## Chapter One

1- What was the reason why the NoSQL systems emerged and SQL systems are not the best?
* Becuase the traditional data (SQL) systems were not able to hanlde the big amount of data generated by internet, in specific in the moment to scale the Data. Meanwhile SQL database get more and more complex in order to handle it and are not tolerant to human mistakes!

2- What is Horizontal partioning or Sharding and how does it work? 
* It's the use of multiple database servers that are spread the table around the whole servers. In specific this technique spreads the write load across multiple machines. 

### Desired Properties of Big Data System

3- Why is important to make a Big Data System Robust and again faul tolerance?
* In this case is avoid complexities in the data system making this simple including in them the immutability into the core of the Big Data System that become them innately resilient to human error by providing a clear and simple mechanusm for recovery.

4- Why is important to have low latency reads and updates in the system and how it has relation with the Scalability propierty?
* Because the system need to be able to update and propagate immediately (in a few milliseconds to few hundred) the information (or not according to the necesities) but without compromising the robustness of the system as the same time the system need to maintin performance in the face of increasing data or load by adding resources to the system (Horizontable scalable) without increase the latency.

5- Why the system need to have generalization and extensibility at the same time? 
* Because it needs to support a wide range of applications allowing an easy large-scale migrations.

6- Why are important the minimal maintenance and debuggability, and how these are related?
* In this case because in order to have a minimal maintenance your algorithms and components need to be simple in order to avoid complexity and with a simple system, is easier to debugg the problems when something goes wrong. 

### Problems with fully incremental architectures

7- What is **Compaction** and why do you need to avoid it?
* It's the action to reclaim space in order to prevent the disk from filling up by the writing and write database. You must avoid it because the reclaiming space become expensive because it has higher demand on the CPU wich dramatically lowers the performance of the machines during a time even with the posibility to cause cascading failure making overload on the rest of the cluster.

8- What does need the Batch Layer be able to do and why does it has great advantages?
* It need to store and immutable, constantly growing master dataset and compute arbitrary fuctions on that dataset. The advantages of the batch layers is that is simple to use and its batch computations are written like single.threaded programs that give you parallelism for free, it's easy to write robust and highly scalablle.

9- What is the Serving Layer, how does it work and what need to support?
* It's a specialized distributed database that load in the batch view automatically swaping when more result are available. This need to support batch updates and random reads.

10- How does work the Speed Layer and what is its difference between the Batch Layer?
* It works producing views based on data it recieves but this only looks at recent data, whereas the batch layer looks at all data at once. In addition to that the speed layer doesn't look at all the new data at once, instead it updates the realtime view as it recieves new data instead of recomputing the whole views

11- How is summarized the Lambda Architecture?
* batch view = function(all data)
* realtime view = function(realtime view, new data)
* query = function(batch view, realtime view)

12- What is *Complexity isolation*?
* It's the capacity to discard  pieces  of  the  realtime  view  as  theyâ€™re  no  longer needed, other words because they are only temporary.

## Chapter 2

1- Why is important to share Raw Data?
* Because with the raw data you maximize yout ability to obtain new insights, whereas summarizing, overwriting or deleting information limits what your data can tell you.

2. 
